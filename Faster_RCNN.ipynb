{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster-RCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd42070bc124440fb6ae91b9dcecd13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b9c5140f02644d818ca8349d1e8c20ab",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7a50fcfb3d345e0a4439d1d969c0c71",
              "IPY_MODEL_58dce14ee71a4f2ba0a3db6a08fe9dad"
            ]
          }
        },
        "b9c5140f02644d818ca8349d1e8c20ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7a50fcfb3d345e0a4439d1d969c0c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68baa4f8dea64dceb3c8eb74e1b2166d",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ec1842392b74bd08738963f554ff7f0"
          }
        },
        "58dce14ee71a4f2ba0a3db6a08fe9dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_246b9b1ded0d4856bf1900ac6ee5836c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:05&lt;00:00, 100MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1dedc91e897a491c974f9287965e846f"
          }
        },
        "68baa4f8dea64dceb3c8eb74e1b2166d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ec1842392b74bd08738963f554ff7f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "246b9b1ded0d4856bf1900ac6ee5836c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1dedc91e897a491c974f9287965e846f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSeUL0IdXN96",
        "colab_type": "text"
      },
      "source": [
        "### Set up google colab and unzip train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwSZ1YsHRx95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up google drive in google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQLVbeXsRz78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Unzip training data from drive\n",
        "\n",
        "!unzip -q 'drive/My Drive/VOCdevkit.zip' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTs4M8P2XDnm",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9zRxDGEsfi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import math\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch.optim as optim\n",
        "from torchvision.ops import nms\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfM1sOVGWTVt",
        "colab_type": "text"
      },
      "source": [
        "### Define Model Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUeu8zmRRj2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b4f3a1cc-5781-426f-a2a3-b9981d37e235"
      },
      "source": [
        "select_classes = {'aeroplane', 'bicycle','boat','bus', 'car','train','motorbike'} # Only training for these classes\n",
        "\n",
        "\n",
        "\n",
        "anchor_scale = torch.FloatTensor([8,16,32])  \n",
        "anchor_ratio = torch.FloatTensor([0.5,1,2])  \n",
        "conversion_scale = 16\n",
        "input_image_height = 800\n",
        "input_image_width = 800\n",
        "num_anchors_sample = 256\n",
        "\n",
        "# Non Maximum Suppression hyper parameters\n",
        "nms_threshold = 0.7       # only consider anchors with IOU < nms_threshold\n",
        "nms_num_train_pre = 12000 # Select top nms_num_train_pre anchors to apply nms on \n",
        "nms_num_train_post = 2000 # number of proposal regions after NMS\n",
        "nms_num_test_pre = 6000   # For test \n",
        "nms_num_test_post = 300   # For test\n",
        "nms_min_size = 16         # only consider anchors as valid while applying nms if ht and wt < nms_min_size\n",
        "\n",
        "\n",
        "#proposal target (pt) hyper parameters\n",
        "pt_n_sample = 128  #Number of samples to sample from roi\n",
        "pt_pos_ratio = 0.25 #the number of positive examples out of the n_samples\n",
        "pt_pos_iou_threshold  = 0.5 #Min overlap required between roi and gt for considering positive label (object)\n",
        "pt_neg_iou_threshold = 0.5 # below this value marks as negative\n",
        "\n",
        "\n",
        "#ROI pooling\n",
        "roi_size = (7,7)\n",
        "\n",
        "# RPN loss \n",
        "rpn_loss_lambda = 1 # same is used in the decider loss\n",
        "\n",
        "\n",
        "# Number of validation images\n",
        "num_valid_img = 150\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDwEFK8tcolt",
        "colab_type": "text"
      },
      "source": [
        "### Handle Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqgc_5G3XYXx",
        "colab_type": "text"
      },
      "source": [
        "##### Get all classes and create label encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tzdBruhSi-Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "391c5938-e2a8-448f-c7e8-c702fb10ce08"
      },
      "source": [
        "# Getting all the classes and creating label encoding\n",
        "\n",
        "all_labels = []\n",
        "\n",
        "\n",
        "for out in sorted(os.listdir('VOCdevkit/VOC2007/Annotations/')):\n",
        "    tree = ET.parse('VOCdevkit/VOC2007/Annotations/' + out)\n",
        "    for obj in tree.findall('object'):\n",
        "        lab = (obj.find('name').text)\n",
        "        # all_labels.append(lab)\n",
        "        if (lab in (select_classes)):\n",
        "          all_labels.append(lab)\n",
        "        \n",
        "distict_labels = list(set(all_labels))\n",
        "\n",
        "#label 0 is set for background\n",
        "lab_to_val = {j:i+1 for i,j in enumerate(distict_labels)}\n",
        "val_to_lab = {i+1:j for i,j in enumerate(distict_labels)}\n",
        "\n",
        "num_classes = len(distict_labels) + 1\n",
        "\n",
        "\n",
        "\n",
        "print(num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u01K3tCSXgQI",
        "colab_type": "text"
      },
      "source": [
        "##### Create Pytorch Dataset and Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xcLsN7xSlxI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class pascal_voc_data(Dataset):\n",
        "    def __init__(self, img_dir,desc_dir, transform = None):\n",
        "        super().__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.desc_dir = desc_dir\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.img_names = sorted(os.listdir(img_dir))\n",
        "        self.img_descs = sorted(os.listdir(desc_dir))\n",
        "        \n",
        "        self.img_names = [os.path.join(img_dir, img_name) for img_name in self.img_names]\n",
        "        self.img_descs = [os.path.join(desc_dir, img_desc) for img_desc in self.img_descs]\n",
        "                \n",
        "        \n",
        "        self.loc_gts = []\n",
        "        self.loc_labels = []\n",
        "        self.final_img_names = []\n",
        "        for img_idx,img_desc in enumerate(self.img_descs):\n",
        "            tree = ET.parse(img_desc)\n",
        "            gt = []\n",
        "            loc_lab = []\n",
        "            for obj in tree.findall('object'):\n",
        "              if ((obj.find('name').text) not in (select_classes)):\n",
        "                continue\n",
        "              lab = lab_to_val[(obj.find('name').text)]\n",
        "              \n",
        "              loc1 = int(obj.find('bndbox').find('xmin').text)\n",
        "              loc2 = int(obj.find('bndbox').find('xmax').text)\n",
        "              loc3 = int(obj.find('bndbox').find('ymin').text)\n",
        "              loc4 = int(obj.find('bndbox').find('ymax').text)\n",
        "\n",
        "              # if ht or width is less than 10, ignore the gt box\n",
        "              if ((loc2 - loc1) < 10 ) or ((loc4 - loc3) < 10):\n",
        "                continue\n",
        "\n",
        "              gt.append([int(loc1),int(loc2),int(loc3),int(loc4)])\n",
        "              loc_lab.append(lab)\n",
        "            if (len(gt) == 0):\n",
        "              continue\n",
        "            self.loc_gts.append(gt)\n",
        "            self.loc_labels.append(loc_lab)\n",
        "            self.final_img_names.append(self.img_names[img_idx])\n",
        "\n",
        "        self.img_names = self.final_img_names\n",
        "             \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        img_name = self.img_names[idx]\n",
        "        img = Image.open(img_name)\n",
        "        \n",
        "        img_h_pre = img.size[1]\n",
        "        img_w_pre = img.size[0]\n",
        "        \n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "            \n",
        "        img_h_post = img.shape[1]\n",
        "        img_w_post = img.shape[2]\n",
        "        \n",
        "        height_ratio = img_h_post/img_h_pre\n",
        "        width_ratio = img_w_post/img_w_pre\n",
        "      \n",
        "        arr_loc_gts = np.array(self.loc_gts[idx])\n",
        "        \n",
        "        arr_loc_gts[:,0] = arr_loc_gts[:,0]*width_ratio\n",
        "        arr_loc_gts[:,1] = arr_loc_gts[:,1]*width_ratio\n",
        "        arr_loc_gts[:,2] = arr_loc_gts[:,2]*height_ratio\n",
        "        arr_loc_gts[:,3] = arr_loc_gts[:,3]*height_ratio\n",
        "                        \n",
        "        gts = (arr_loc_gts).tolist()\n",
        "        label = self.loc_labels[idx]\n",
        "        \n",
        "        return img, gts,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smlTcDZ1Sz1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "While using pretrained models - \n",
        "Pytorch torchvision documentation - https://pytorch.org/docs/master/torchvision/models.html\n",
        "The images have to be loaded in to a range of [0, 1] and then \n",
        "normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]\n",
        "'''\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((input_image_height,input_image_width)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "inv_normalize = transforms.Normalize(\n",
        "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "   std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IOlskQ3S8Zx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b73958e0-9f77-4d66-e9d6-653fc76c8183"
      },
      "source": [
        "# Training and validation dataLoader\n",
        "\n",
        "data = pascal_voc_data('VOCdevkit/VOC2007/JPEGImages/', 'VOCdevkit/VOC2007/Annotations/', transform)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "train, valid = random_split(data, [len(data)-num_valid_img, num_valid_img])\n",
        "torch.manual_seed(torch.initial_seed())\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=1)\n",
        "valid_loader = DataLoader(valid, batch_size=1)\n",
        "\n",
        "\n",
        "num_train_imgs = len(train_loader)\n",
        "print(len(train_loader))\n",
        "print(len(valid_loader))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1791\n",
            "150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng_gQP2XX5WK",
        "colab_type": "text"
      },
      "source": [
        "### Visualise input image with box co-ordinates provided in format - x0,x1,y0,y1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBSCEoTTTAAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Given input image, draw rectangles as specified by gt_box and pred_box and display\n",
        "def visualize_tensor(img, gt_box, pred_box,save_image='',tb_writer=None):\n",
        "    plt.figure(figsize=(5,5))\n",
        "    transform_img = inv_normalize(img[0]).permute(1,2,0).to('cpu').numpy()\n",
        "    transform_img = transform_img.copy()\n",
        "    for box in gt_box:\n",
        "        x0, x1, y0, y1 = box\n",
        "        cv2.rectangle(transform_img, (int(x0),int(y0)), (int(x1),int(y1)), color=(0, 255, 255), thickness=2)\n",
        "    for box in pred_box:\n",
        "        x0, x1, y0, y1 = box\n",
        "        cv2.rectangle(transform_img, (int(x0), int(y0)), (int(x1), int(y1)), color=(255, 0, 0), thickness=2)\n",
        "    \n",
        "    if tb_writer:\n",
        "      # grid = torchvision.utils.make_grid(transform_img)\n",
        "      tb_writer.add_image(save_image, transform_img, dataformats='HWC')\n",
        "    elif save_image == '':\n",
        "        plt.imshow(transform_img)\n",
        "        plt.show()  \n",
        "    else:\n",
        "        plt.imshow(transform_img)\n",
        "        plt.savefig(save_image + '.png')\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87N6zHd_YFh7",
        "colab_type": "text"
      },
      "source": [
        "### Faster RCNN backbone used - VGG16 model\n",
        "\n",
        "*   Top 10 layers (top 4 conv layers) are not trained\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUkDSEqgTLvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Base VGG16 model\n",
        "\n",
        "def Faster_RCNN_vgg16(num_freeze_top): \n",
        "    vgg16 = models.vgg16(pretrained=True)\n",
        "    vgg_feature_extracter  = vgg16.features[:-1]\n",
        "    vgg_classifier = vgg16.classifier[:-1]\n",
        "    \n",
        "    # Freeze learning of top few conv layers\n",
        "    for layer in vgg_feature_extracter[:num_freeze_top]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "    \n",
        "    return vgg_feature_extracter.to(device), vgg_classifier.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkuFUER3YUGh",
        "colab_type": "text"
      },
      "source": [
        "### Code for anchor generation on an image\n",
        "\n",
        "*   For train and test, only anchors lying inside the image boundary are considered\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeyisFcbTfvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    RCNN Paper\n",
        "    For anchors, we use 3 scales with box areas of (128*128) || (256*256) , and (512*512) pixels, \n",
        "    and 3 aspect ratios of 1:1, 1:2, and 2:1.\n",
        "\n",
        "'''\n",
        "\n",
        "# At point x,y from feature map, return 9 anchors on the input image scale\n",
        "def generate_anchor_at_point(x,y):\n",
        "    anchor_positions = torch.zeros((len(anchor_scale) * len(anchor_ratio),4))\n",
        "    ctr_x = (x*2+1)*(conversion_scale/2)   # for x = 0, centre is 8 || for x = 1, centre is 24\n",
        "    ctr_y = (y*2+1)*(conversion_scale/2)\n",
        "    for ratio_idx in range(len(anchor_ratio)):\n",
        "        for scale_idx in range(len(anchor_scale)):\n",
        "\n",
        "            current = len(anchor_scale)*ratio_idx + scale_idx\n",
        "            ratio = anchor_ratio[ratio_idx]\n",
        "            scale = anchor_scale[scale_idx]\n",
        "\n",
        "            h = conversion_scale*scale*torch.sqrt(ratio)\n",
        "            w = conversion_scale*scale*torch.sqrt(1.0/ratio)\n",
        "            \n",
        "            anchor_positions[current,0] = ctr_x - w/2\n",
        "            anchor_positions[current,1] = ctr_x + w/2\n",
        "            anchor_positions[current,2] = ctr_y - h/2\n",
        "            anchor_positions[current,3] = ctr_y + h/2\n",
        "            \n",
        "    return anchor_positions\n",
        "\n",
        "\n",
        "# For features of scale (x,y) , generate all the anchor boxes\n",
        "# input is height,width\n",
        "# returns output on  x*y*9,4\n",
        "def generate_anchors(x,y):\n",
        "    anchor_positions = torch.zeros((x*y,len(anchor_scale) * len(anchor_ratio),4))\n",
        "    for ctr_x in range(x):\n",
        "        for ctr_y in range(y):\n",
        "            current = ctr_x*y + ctr_y\n",
        "            anchors = generate_anchor_at_point(ctr_x, ctr_y)\n",
        "            anchor_positions[current] = anchors\n",
        "    return anchor_positions.reshape(-1,4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJh4ImOsTq2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "RCNN paper\n",
        "    - During training, we ignore all cross-boundary anchors so they do not contribute to the loss\n",
        "    - During testing, however, we still apply the fully convolutional RPN to the entire image. This may \n",
        "    generate crossboundary proposal boxes, which we clip to the image boundary\n",
        "'''\n",
        "\n",
        "'''\n",
        "    For this code, cross boundary anchors are ignored at this step both in train and test\n",
        "'''\n",
        "\n",
        "def get_valid_anchors(anchor_positions):\n",
        "    valid_anchors_idx = torch.where((anchor_positions[:,0] >= 0) & \n",
        "             (anchor_positions[:,1] <= input_image_width) &\n",
        "             (anchor_positions[:,2] >= 0) &\n",
        "             (anchor_positions[:,3] <= input_image_height) )[0]\n",
        "\n",
        "    anchor_positions = anchor_positions[valid_anchors_idx]\n",
        "    return anchor_positions, valid_anchors_idx\n",
        "    \n",
        "# valid_anchors,valid_anchors_idx = get_valid_anchors(anchor_positions)\n",
        "# print(valid_anchors)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSyI0-x9Yves",
        "colab_type": "text"
      },
      "source": [
        "### Getting intersection over union between 2 sets of boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYmH88-_U9yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Intersection over Union between all the boxes in anchor_positions and gt_boxes\n",
        "\n",
        "def get_iou_matrix(anchor_positions, gt_boxes):\n",
        "    iou_matrix = torch.zeros((len(anchor_positions), len(gt_boxes)))\n",
        "    for idx,box in enumerate(gt_boxes):\n",
        "        if isinstance(box,torch.Tensor):\n",
        "          gt = torch.cat([box]*len(anchor_positions)).view(1,-1,4)[0]\n",
        "        else:\n",
        "          gt = torch.FloatTensor([box]*len(anchor_positions))\n",
        "        max_x = torch.max(gt[:,0],anchor_positions[:,0])\n",
        "        min_x = torch.min(gt[:,1],anchor_positions[:,1])\n",
        "        max_y = torch.max(gt[:,2],anchor_positions[:,2])\n",
        "        min_y = torch.min(gt[:,3],anchor_positions[:,3])\n",
        "                \n",
        "        invalid_roi_idx = (min_x < max_x) | (min_y < max_y)\n",
        "        roi_area = (min_x - max_x)*(min_y - max_y)\n",
        "        roi_area[invalid_roi_idx] = 0\n",
        "        \n",
        "        total_area = (gt[:,1] - gt[:,0])*(gt[:,3] - gt[:,2]) + \\\n",
        "                    (anchor_positions[:,1] - anchor_positions[:,0])*(anchor_positions[:,3]-anchor_positions[:,2]) - \\\n",
        "                     roi_area\n",
        "                    \n",
        "        iou = roi_area/(total_area + 1e-6)\n",
        "        \n",
        "        iou_matrix[:,idx] = iou\n",
        "    return iou_matrix\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUAuHRqFZFbP",
        "colab_type": "text"
      },
      "source": [
        "### After IOU calculation between anchors and gt boxes \n",
        "\n",
        "*   Assign +1/-1/0 labels to anchors based on IOU\n",
        "*   Sample 128 positive and 128 negative anchors for training\n",
        "* If less than 128 positive anchors, pad with negative\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_fvWDP7dHDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "    RCNN paper: \n",
        "        assign a positive label to two kinds of anchors: \n",
        "        (i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, \n",
        "        (ii) an anchor that has an IoU overlap higher than 0.7 with\n",
        "    \n",
        "        assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 \n",
        "        Anchors that are neither positive nor negative do not contribute to the training objective.\n",
        "        \n",
        "'''\n",
        "\n",
        "def get_max_iou_data(iou_matrix):\n",
        "    gt_max_value = iou_matrix.max(axis=0)[0] #  for each gt box, this is the max iou\n",
        "    \n",
        "    #There is a possiblilty that corresponding to one gt box, there are multiple anchors with same iou (max value)\n",
        "    all_gt_max = torch.where(iou_matrix == gt_max_value)[0]\n",
        "    \n",
        "    # For each anchor box, this is the max iou with any of the gt_box\n",
        "    anchor_max_value = torch.max(iou_matrix, axis=1)[0]\n",
        "    anchor_max = torch.argmax(iou_matrix, axis=1)\n",
        "    \n",
        "    return all_gt_max, anchor_max_value,anchor_max\n",
        "\n",
        "\n",
        "\n",
        "# 1 - positive || 0 - negative || -1 - ignore\n",
        "def get_anchor_labels(anchor_positions, all_gt_max, anchor_max_value):\n",
        "    anchor_labels = torch.zeros(anchor_positions.shape[0])\n",
        "    anchor_labels.fill_(-1.0)\n",
        "\n",
        "    # for each anchor box, if iou with any of the gt_box is less than threshold, mark as 0\n",
        "    anchor_labels[anchor_max_value < 0.3] = 0\n",
        "    \n",
        "    # If corresponding to any gt_box, the anchor has max iou -> mark as 1\n",
        "    anchor_labels[all_gt_max] = 1.0\n",
        "    \n",
        "    # If for any anchor box, iou is greater than threshold for any of the gt_box, mark as 1\n",
        "    anchor_labels[anchor_max_value > 0.7] = 1.0\n",
        "    return anchor_labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEJvEvsdPZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        " RCNN paper \n",
        "    randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where\n",
        "    the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer \n",
        "    than 128 positive samples in an image, we pad the mini-batch with negative ones.\n",
        "'''\n",
        "\n",
        "def sample_anchors_for_train(anchor_labels):\n",
        "    pos_anchor_labels = torch.where(anchor_labels == 1)[0]\n",
        "    num_pos = min(num_anchors_sample/2, len(pos_anchor_labels))\n",
        "    pos_idx = np.random.choice(pos_anchor_labels,  int(num_pos), replace=False)\n",
        "\n",
        "    neg_anchor_labels = torch.where(anchor_labels == 0)[0]\n",
        "    num_neg = num_anchors_sample - num_pos\n",
        "    neg_idx = np.random.choice(neg_anchor_labels, int(num_neg), replace=False)\n",
        "    \n",
        "    anchor_labels[:] = -1\n",
        "    anchor_labels[pos_idx] = 1\n",
        "    anchor_labels[neg_idx] = 0\n",
        "    \n",
        "    return anchor_labels\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9WlodDVZiAp",
        "colab_type": "text"
      },
      "source": [
        "### bbox regression training data calculation\n",
        "* delta_anchor_gt - Get delta between anchor_positions and gt_boxes\n",
        "* correct_anchor_positions - Given delta and anchor_positions, correct the anchors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uwFrcHmdQB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    x,y,w,h are ctr_x, ctr_y, width and height for GT box\n",
        "    dx = (x - x_{a})/w_{a}\n",
        "    dy = (y - y_{a})/h_{a}\n",
        "    dw = log(w/ w_a)\n",
        "    dh = log(h/ h_a)\n",
        "'''\n",
        "\n",
        "# Get delta between anchor_positions and gt_boxes\n",
        "def delta_anchor_gt(anchor_positions, gt_boxes , anchor_max):\n",
        "\n",
        "    anchor_gt_map =  gt_boxes[anchor_max]\n",
        "    \n",
        "    anchor_height = anchor_positions[:,3] - anchor_positions[:,2] # y2-y1\n",
        "    anchor_width =  anchor_positions[:,1] - anchor_positions[:,0] # x2-x1\n",
        "    anchor_ctr_y = anchor_positions[:,2] + anchor_height/2 # y1 + h/2\n",
        "    anchor_ctr_x = anchor_positions[:,0] + anchor_width/2  # x1 + w/2\n",
        "    \n",
        "    gt_height = anchor_gt_map[:,3] - anchor_gt_map[:,2] # y2-y1\n",
        "    gt_width =  anchor_gt_map[:,1] - anchor_gt_map[:,0] # x2-x1\n",
        "    gt_ctr_y = anchor_gt_map[:,2] + gt_height/2 # y1 + h/2\n",
        "    gt_ctr_x = anchor_gt_map[:,0] + gt_width/2  # x1 + w/2\n",
        "    \n",
        "    dx = (gt_ctr_x - anchor_ctr_x)/anchor_width\n",
        "    dy = (gt_ctr_y - anchor_ctr_y)/anchor_height\n",
        "    dw = torch.log(gt_width/anchor_width)\n",
        "    dh = torch.log(gt_height/anchor_height)\n",
        "    \n",
        "    delta = torch.zeros_like(anchor_positions)\n",
        "    delta[:,0] = dx\n",
        "    delta[:,1] = dy\n",
        "    delta[:,2] = dw\n",
        "    delta[:,3] = dh\n",
        "   \n",
        "    return delta\n",
        "\n",
        "\n",
        "# Given delta and anchor_positions, correct the anchors\n",
        "def correct_anchor_positions(anchor_positions, delta):\n",
        "    ha = anchor_positions[:,3] - anchor_positions[:,2] # y2-y1\n",
        "    wa =  anchor_positions[:,1] - anchor_positions[:,0] # x2-x1\n",
        "    ya = anchor_positions[:,2] + ha/2 # y1 + h/2\n",
        "    xa = anchor_positions[:,0] + wa/2  # x1 + w/2\n",
        "    \n",
        "    dx = delta[:,0]\n",
        "    dy = delta[:,1]\n",
        "    dw = delta[:,2]\n",
        "    dh = delta[:,3]\n",
        "    \n",
        "    \n",
        "    x = dx*wa + xa\n",
        "    y = dy*ha + ya\n",
        "    w = torch.exp(dw)*wa\n",
        "    h = torch.exp(dh)*ha\n",
        "    \n",
        "    correct_anchor_positions = torch.zeros_like(anchor_positions)\n",
        "    \n",
        "    correct_anchor_positions[:,0] = x - w/2\n",
        "    correct_anchor_positions[:,1] = x + w/2\n",
        "    correct_anchor_positions[:,2] = y - h/2\n",
        "    correct_anchor_positions[:,3] = y + h/2\n",
        "    \n",
        "    return correct_anchor_positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77rqSE9zZ5yq",
        "colab_type": "text"
      },
      "source": [
        "### Faster RCNN region proposal network defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbYuoWk7x1xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Region Proposal Network\n",
        "\n",
        "class Faster_RCNN_rpn(nn.Module):\n",
        "    def __init__(self,extracter):\n",
        "        super().__init__()\n",
        "        self.extracter = extracter\n",
        "        self.conv1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        #class_conv1 checks corresponding to 1 point in feature map, 18 outputs. \n",
        "        # 9 anchors * (2) || anchor has object or not\n",
        "        self.class_conv = nn.Conv2d(512, 2*len(anchor_scale)*len(anchor_ratio), 1, 1, 0)  \n",
        "        #reg_conv1 checks corresponding to 1 point in feature map, 36 outputs. \n",
        "        # 9 anchors * (4) || anchor delta wrt ground truth boxes\n",
        "        self.reg_conv = nn.Conv2d(512, 4*len(anchor_scale)*len(anchor_ratio), 1 ,1 , 0)\n",
        "\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        self.class_conv.weight.data.normal_(0, 0.01)\n",
        "        self.class_conv.bias.data.zero_()\n",
        "        self.reg_conv.weight.data.normal_(0, 0.01)\n",
        "        self.reg_conv.bias.data.zero_()\n",
        "\n",
        "        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        # input and output features of CNN are (nSamples x nChannels x Height x Width)\n",
        "        \n",
        "        features = self.extracter(x)\n",
        "        conv1_out = F.relu(self.conv1(features))\n",
        "        class_out = self.class_conv(conv1_out)\n",
        "        reg_out = self.reg_conv(conv1_out)\n",
        "        \n",
        "        return features, class_out, reg_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuaRnupHaC2Q",
        "colab_type": "text"
      },
      "source": [
        "### Loss calculation - The same loss function is used for decider and RPN layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRC_TDXZx_hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss for Region Proposal Netwoek\n",
        "# Same loss is used for decider network\n",
        "\n",
        "def RPN_loss(locs_preditct, class_predict, final_RPN_locs,final_RPN_class):\n",
        "    final_RPN_locs = final_RPN_locs.to(device)\n",
        "    final_RPN_class = final_RPN_class.long().to(device)\n",
        "    \n",
        "    # Cross entropy loss (check if the target is background or foreground)\n",
        "    # Only consider labels with values 1 or 0. ignore -1\n",
        "    class_loss = F.cross_entropy(class_predict, final_RPN_class, ignore_index=-1)\n",
        "\n",
        "    #smooth L1 regression loss (calculate the loss in predicted locations of foreground)\n",
        "    '''\n",
        "        Smooth L1 loss uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise\n",
        "    '''\n",
        "\n",
        "    foreground_class_idx = (final_RPN_class > 0)\n",
        "    locs_preditct  = locs_preditct[foreground_class_idx]\n",
        "    final_RPN_locs = final_RPN_locs[foreground_class_idx]\n",
        "    \n",
        "    loc_loss = F.smooth_l1_loss(locs_preditct, final_RPN_locs) / (sum(foreground_class_idx)+1e-6)\n",
        "\n",
        "    \n",
        "    rpn_loss = class_loss + rpn_loss_lambda*loc_loss\n",
        "  \n",
        "    return rpn_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LAvoeviaKLd",
        "colab_type": "text"
      },
      "source": [
        "### Non Maximum Suppression\n",
        "* Custom implemented method is implemented but not used\n",
        "* Pytorch nms method gives ~5-6 times speed up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZPXzkHbdUai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply non max suppression on anchors\n",
        "\n",
        "def non_max_suppression(correct_anchor_positions, class_score, img_h, img_w, isTrain):\n",
        "    \n",
        "    if isTrain:\n",
        "        nms_pre = nms_num_train_pre\n",
        "        nms_post = nms_num_train_post\n",
        "    else :\n",
        "        nms_pre = nms_num_test_pre\n",
        "        nms_post = nms_num_test_post\n",
        "    \n",
        "    \n",
        "    # Clip the anchors to image dimensions\n",
        "    correct_anchor_positions[correct_anchor_positions[:,0] < 0,0] = 0 # x1\n",
        "    correct_anchor_positions[correct_anchor_positions[:,1] > img_w,1] = img_w # x2\n",
        "    correct_anchor_positions[correct_anchor_positions[:,2] < 0,2] = 0 # y1\n",
        "    correct_anchor_positions[correct_anchor_positions[:,3] > img_h,3] = img_h # y2\n",
        "    \n",
        "    \n",
        "    # Only keep anchors with height and width > nms_min_size\n",
        "    anchor_width = correct_anchor_positions[:,1] - correct_anchor_positions[:,0]\n",
        "    anchor_height = correct_anchor_positions[:,3] - correct_anchor_positions[:,2]\n",
        "\n",
        "\n",
        "    \n",
        "    keep_idx = (anchor_height > nms_min_size) & (anchor_width > nms_min_size)\n",
        "    correct_anchor_positions = correct_anchor_positions[keep_idx]\n",
        "    class_score = class_score[keep_idx]\n",
        "\n",
        "    # Get the index of sorted class scores in descending order and select top nms_pre idx\n",
        "    sorted_class_scores = torch.argsort(class_score, descending=True)\n",
        "    pre_nms_idx = sorted_class_scores[:nms_pre]\n",
        "    correct_anchor_positions = correct_anchor_positions[pre_nms_idx]\n",
        "    class_score = class_score[pre_nms_idx]\n",
        "    \n",
        "    # Implementation for non max suppression\n",
        "    '''\n",
        "    sorted_class_scores = torch.argsort(class_score, descending=True).to('cpu')\n",
        "    keep_anchors = []\n",
        "    \n",
        "\n",
        "    Apply NMS\n",
        "    while len(sorted_class_scores) > 1:\n",
        "        current = sorted_class_scores[0]\n",
        "        keep_anchors.append(current)\n",
        "        iou_matrix = get_iou_matrix(correct_anchor_positions[sorted_class_scores[1:]],correct_anchor_positions[current].reshape(1,-1,4)[0])\n",
        "        sorted_class_scores = sorted_class_scores[np.where(iou_matrix < nms_threshold)[0] + 1]\n",
        "    \n",
        "    if (len(sorted_class_scores) == 1):\n",
        "        keep_anchors.append(sorted_class_scores[0])\n",
        "    '''\n",
        "\n",
        "\n",
        "    '''\n",
        "      using pytorch standard nms function as it gives 5-6 times speedup\n",
        "    '''\n",
        "    change_format = torch.zeros_like(correct_anchor_positions)\n",
        "    change_format[:,0] = correct_anchor_positions[:,0]\n",
        "    change_format[:,1] = correct_anchor_positions[:,2]\n",
        "    change_format[:,2] = correct_anchor_positions[:,1]\n",
        "    change_format[:,3] = correct_anchor_positions[:,3]\n",
        "\n",
        "    keep_anchors = nms(change_format.to('cpu'), class_score.clone().detach().to('cpu'), nms_threshold)\n",
        "\n",
        "    keep_anchors = keep_anchors[:nms_post]\n",
        "    correct_anchor_positions = correct_anchor_positions[keep_anchors]\n",
        "    \n",
        "    return correct_anchor_positions\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwtmTUzacBH",
        "colab_type": "text"
      },
      "source": [
        "### Assign classes to output of Region Proposal Network "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVDS5ryzh6PG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def assign_classification_anchors(extracted_roi, gt_boxes, gt_labels, isTrain):\n",
        "\n",
        "    \n",
        "    # calculate iou of rois and gt boxes\n",
        "    iou_matrix = get_iou_matrix(extracted_roi, gt_boxes)\n",
        "    \n",
        "    # for each ROI, find gt with max iou and corresponding value\n",
        "    gt_roi_argmax = iou_matrix.argmax(axis=1)\n",
        "    gt_roi_max = iou_matrix.max(axis=1)[0]\n",
        "    \n",
        "    #If a particular ROI has max overlap with a gt_box, assign label of gt_box to roi\n",
        "    assign_labels = gt_labels[gt_roi_argmax]\n",
        "    \n",
        "    num_pos = pt_n_sample*pt_pos_ratio\n",
        "    pos_idx = torch.where(gt_roi_max > pt_pos_iou_threshold)[0]\n",
        "    if isTrain:\n",
        "      pos_idx = np.random.choice(pos_idx, int(min(len(pos_idx), num_pos)), replace=False)\n",
        "    \n",
        "    \n",
        "    num_neg = pt_n_sample - len(pos_idx)\n",
        "    neg_idx = torch.where(gt_roi_max < pt_neg_iou_threshold)[0]\n",
        "    if isTrain:\n",
        "      neg_idx = np.random.choice(neg_idx, int(min(len(neg_idx), num_neg)), replace=False)\n",
        "    \n",
        "    \n",
        "    keep_idx = np.append(pos_idx, neg_idx)\n",
        "    assign_labels[neg_idx] = 0\n",
        "    assign_labels = assign_labels[keep_idx]\n",
        "    extracted_roi = extracted_roi[keep_idx]\n",
        "    gt_roi_argmax = gt_roi_argmax[keep_idx]\n",
        "\n",
        "    \n",
        "    return assign_labels, extracted_roi,gt_roi_argmax, keep_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-1clxYZamee",
        "colab_type": "text"
      },
      "source": [
        "### ROI Pooling Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zBYJHMOaZi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ROI_pooling(extracted_roi, feature,ROI_pooling_layer):\n",
        "    extracted_roi = extracted_roi/16.0\n",
        "    out = []\n",
        "    for roi in extracted_roi:\n",
        "        \n",
        "        x1 = int(roi[0])\n",
        "        x2 = int(roi[1]+1)\n",
        "        y1 = int(roi[2])\n",
        "        y2 = int(roi[3]+1)\n",
        "        out.append(ROI_pooling_layer(feature[:,:,y1:y2,x1:x2]))\n",
        "    out = torch.cat(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwYUmNDOaqPB",
        "colab_type": "text"
      },
      "source": [
        "### Faster RCNN decider layer definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmDFngjsiJ43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Faster_RCNN_decider(nn.Module):\n",
        "    def __init__(self,classifier):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.classifier = classifier\n",
        "        self.class_lin = nn.Linear(4096, num_classes)\n",
        "        self.reg_lin = nn.Linear(4096, num_classes*4)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.classifier(x)\n",
        "        decider_class = self.class_lin(x)\n",
        "        decider_loc = self.reg_lin(x)\n",
        "        \n",
        "        return decider_class, decider_loc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynpLMBOSauXS",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline (train/validation) for a single image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hs58JpWiNlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "img_anchors_all = generate_anchors(int(input_image_height/conversion_scale), int(input_image_width/conversion_scale)).to(device)\n",
        "\n",
        "def single_image_pipeline(input_image, gt_box, label,isTrain):\n",
        "\n",
        "    input_image = input_image.to(device)\n",
        "    gt_box = torch.FloatTensor(gt_box).to(device)\n",
        "    label = torch.FloatTensor(label).to(device)\n",
        "\n",
        "    # Generate CNN features for input image\n",
        "    # Genearate region proposals predictions\n",
        "    features, class_out, reg_out = rpn(input_image)\n",
        "\n",
        "\n",
        "    locs_preditct = reg_out.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
        "    class_predict = class_out.permute(0,2,3,1).reshape(1,-1,2)[0]\n",
        "    class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
        "    \n",
        "   \n",
        "    # For training region proposal network, generate anchors on the image. \n",
        "    img_anchors_valid, img_anchors_valid_idx = get_valid_anchors(img_anchors_all.clone())\n",
        "\n",
        "    iou_anchors_gt = get_iou_matrix(img_anchors_valid, gt_box)\n",
        "    all_gt_max, anchor_max_value,anchor_max = get_max_iou_data(iou_anchors_gt)\n",
        "    \n",
        "\n",
        "    anchor_labels = get_anchor_labels(img_anchors_valid, all_gt_max, anchor_max_value)\n",
        "    anchor_labels = sample_anchors_for_train(anchor_labels)\n",
        "    \n",
        "    # TODO - check if correct. - Done\n",
        "    delta = delta_anchor_gt(img_anchors_valid, gt_box, anchor_max)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    final_RPN_locs = torch.zeros_like(img_anchors_all)\n",
        "    final_RPN_locs[img_anchors_valid_idx] = delta\n",
        "\n",
        "    final_RPN_class = torch.zeros(img_anchors_all.shape[0])\n",
        "    final_RPN_class.fill_(-1)\n",
        "    final_RPN_class[img_anchors_valid_idx] = anchor_labels\n",
        "\n",
        "    # Loss for RPN layer\n",
        "    loss1 = RPN_loss(locs_preditct, class_predict, final_RPN_locs, final_RPN_class).to(device)\n",
        "    \n",
        "    # Based on the bbox output of rpn, correct the generated anchors\n",
        "    corrected_anchors = correct_anchor_positions(img_anchors_all.to(device), locs_preditct).detach()\n",
        "\n",
        "    \n",
        "    \n",
        "    # Apply nms on the region proposals\n",
        "    extracted_rois = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3],isTrain)\n",
        "\n",
        "   \n",
        "    final_decider_class, extracted_roi_samples,gt_roi_argmax,idx = assign_classification_anchors(extracted_rois, gt_box, label, isTrain)\n",
        "\n",
        "    final_decider_locs = delta_anchor_gt(extracted_roi_samples, gt_box,gt_roi_argmax)\n",
        "   \n",
        "    # Apply ROI pooling on the extracted ROIs\n",
        "    pooled_features = ROI_pooling(extracted_roi_samples, features, ROI_pooling_layer)\n",
        "\n",
        "    decider_class, decider_loc = decider(pooled_features)\n",
        "    decider_loc = decider_loc.reshape(pooled_features.shape[0],-1,4) # 128*21*4\n",
        "    decider_loc = decider_loc[torch.arange(0,pooled_features.shape[0]), final_decider_class.long()] # 128*4\n",
        "    \n",
        "    # Loss for decider layer\n",
        "    loss2 = RPN_loss(decider_loc, decider_class, final_decider_locs, final_decider_class).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        decider_loc_no_grad = decider_loc.clone().to(device)\n",
        "\n",
        "    # Correct the ROIs based on bbox output for decider layers\n",
        "    corrected_roi = correct_anchor_positions(extracted_roi_samples,decider_loc_no_grad).detach()\n",
        "    \n",
        "    return loss1, loss2 , decider_class, corrected_roi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtmTEll_YWD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(input_image):\n",
        "  rpn.eval()\n",
        "  decider.eval()\n",
        "\n",
        "  input_image = input_image.to(device)\n",
        "  features, class_out, reg_out = rpn(input_image)\n",
        "  locs_preditct = reg_out.permute(0,2,3,1).reshape(1,-1,4)[0]\n",
        "  class_score = class_out.reshape(1,features.shape[2],features.shape[3],9,2)[:,:,:,:,1].reshape(1,-1)[0].detach()\n",
        "\n",
        "  corrected_anchors = correct_anchor_positions(img_anchors_all.to(device), locs_preditct).detach()\n",
        "\n",
        "    \n",
        "  extracted_rois = non_max_suppression(corrected_anchors, class_score, input_image.shape[2],input_image.shape[3],False)\n",
        "\n",
        "\n",
        "  pooled_features = ROI_pooling(extracted_rois, features, ROI_pooling_layer)\n",
        "\n",
        "  decider_class, decider_loc = decider(pooled_features)\n",
        "  with torch.no_grad():\n",
        "        decider_loc_no_grad = decider_loc.clone().to(device)\n",
        "\n",
        "  corrected_roi = correct_anchor_positions(extracted_rois,decider_loc_no_grad).detach()\n",
        "\n",
        "  for pred in decider_class.argmax(axis=1)[decider_class.argmax(axis=1) != 0]:\n",
        "        print(val_to_lab[int(pred)], end=', ')\n",
        "\n",
        "  visualize_tensor(input_image,corrected_roi[decider_class.argmax(axis=1) != 0], [])\n",
        "\n",
        "  rpn.train()\n",
        "  decider.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI1Y_uq5ayp9",
        "colab_type": "text"
      },
      "source": [
        "### Use a single image pipeline multiple times for training or batch testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFCdQQ-ybL9Y",
        "colab_type": "text"
      },
      "source": [
        "##### Define variables, models, load saved models or training steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcWUbnTVbfNS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "bd42070bc124440fb6ae91b9dcecd13b",
            "b9c5140f02644d818ca8349d1e8c20ab",
            "c7a50fcfb3d345e0a4439d1d969c0c71",
            "58dce14ee71a4f2ba0a3db6a08fe9dad",
            "68baa4f8dea64dceb3c8eb74e1b2166d",
            "9ec1842392b74bd08738963f554ff7f0",
            "246b9b1ded0d4856bf1900ac6ee5836c",
            "1dedc91e897a491c974f9287965e846f"
          ]
        },
        "outputId": "ba89d287-c7f2-4de8-f30e-44b943502d1d"
      },
      "source": [
        "load_model = 'drive/My Drive/saved_models/epoch_5.pt'\n",
        "\n",
        "\n",
        "\n",
        "loss1_hist = []\n",
        "loss2_hist = []\n",
        "loss_hist = []\n",
        "valid_loss1_hist = []\n",
        "valid_loss2_hist = []\n",
        "valid_loss_hist = []\n",
        "epoch_start = 0\n",
        "iteration_start = 0\n",
        "\n",
        "\n",
        "vgg_feature_extracter, vgg_classifier = Faster_RCNN_vgg16(num_freeze_top=10)\n",
        "rpn = Faster_RCNN_rpn(vgg_feature_extracter).to(device)\n",
        "ROI_pooling_layer = nn.AdaptiveMaxPool2d(roi_size).to(device)\n",
        "decider = Faster_RCNN_decider(vgg_classifier).to(device)\n",
        "all_params = list(list(rpn.parameters()) + list(decider.parameters()))\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(all_params, lr=0.00001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if load_model != '':\n",
        "  print('loading model ... ')\n",
        "  checkpoint = torch.load(load_model, map_location=device)\n",
        "  rpn.load_state_dict(checkpoint['rpn_state_dict'])\n",
        "  decider.load_state_dict(checkpoint['decider_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  loss1_hist = checkpoint['loss1_hist']\n",
        "  loss2_hist = checkpoint['loss2_hist']\n",
        "  loss_hist = checkpoint['loss_hist']\n",
        "  valid_loss1_hist =checkpoint['valid_loss1_hist']\n",
        "  valid_loss2_hist = checkpoint['valid_loss2_hist']\n",
        "  valid_loss_hist = checkpoint['valid_loss_hist']\n",
        "  epoch_start = checkpoint['epoch_start']\n",
        "  iteration_start = checkpoint['iteration_start']\n",
        "  rpn.train()\n",
        "  decider.train()\n",
        "\n",
        "  print('model loaded ...' )\n",
        "\n",
        "\n",
        "running_count = 0\n",
        "running_net_loss = 0\n",
        "running_loss1 = 0\n",
        "running_loss2 = 0\n",
        "loss_avg_step = 20\n",
        "train_visualise_step  =150\n",
        "evaluate_step = 150\n",
        "valid_visualise_step = 150\n",
        "save_step = 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd42070bc124440fb6ae91b9dcecd13b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "loading model ... \n",
            "model loaded ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyQtSvA6bs3t",
        "colab_type": "text"
      },
      "source": [
        "##### Faster RCNN train/validation code (Supported batch size is 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOfnyBNnvWcT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Start epoch - \" + str(epoch_start) + ' and start iteration ' + str(iteration_start))\n",
        "isFirstEpoch = True\n",
        "for epoch in range(epoch_start,epoch_start+15):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    if ((i < iteration_start) & (isFirstEpoch)):\n",
        "      continue\n",
        "    \n",
        "    \n",
        "    \n",
        "    if ((i+1)%save_step == 0):\n",
        "      PATH = 'drive/My Drive/saved_models/epoch_' + str(int(epoch/5)) + '.pt'\n",
        "\n",
        "      print(str(epoch)+ '--' + str(i) + ' saving model ' + PATH)\n",
        "      torch.save({\n",
        "        'rpn_state_dict': rpn.state_dict(),\n",
        "        'decider_state_dict': decider.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss1_hist':loss1_hist,\n",
        "        'loss2_hist':loss2_hist,\n",
        "        'loss_hist':loss_hist,\n",
        "        'valid_loss1_hist': valid_loss1_hist ,\n",
        "        'valid_loss2_hist':valid_loss2_hist,\n",
        "        'valid_loss_hist': valid_loss_hist,\n",
        "        'epoch_start':epoch,\n",
        "        'iteration_start':int(i/loss_avg_step)*loss_avg_step + 1\n",
        "      }, PATH)\n",
        "\n",
        "    \n",
        "    img, gt_box, labels = data\n",
        "\n",
        "    loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, True)\n",
        "    net_loss = loss1 + loss2\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    net_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss1 = loss1.detach()\n",
        "    loss2 = loss2.detach()\n",
        "    pred_class = pred_class.detach()\n",
        "    pred_box = pred_box.detach()\n",
        "    net_loss = net_loss.detach()\n",
        "    if not((math.isnan(net_loss)) or (math.isnan(loss1)) or (math.isnan(loss2))):\n",
        "      running_count += 1\n",
        "      running_net_loss += net_loss.data\n",
        "      running_loss1 += loss1.data\n",
        "      running_loss2 += loss2.data\n",
        "\n",
        "    \n",
        "    if ((i+1)%(loss_avg_step) == 0):\n",
        "      loss1_hist.append(running_loss1/(running_count + 1e-6))\n",
        "      loss2_hist.append(running_loss2/(running_count + 1e-6))\n",
        "      loss_hist.append(running_net_loss/(running_count + 1e-6))\n",
        "      print(str(epoch) + '--' + str(i) + '--- ' + str(running_net_loss/(running_count + 1e-6)))\n",
        "      running_count = 0\n",
        "      running_net_loss = 0\n",
        "      running_loss1 = 0\n",
        "      running_loss2 = 0\n",
        "\n",
        "\n",
        "    if ((i+1)%(train_visualise_step)) == 0:\n",
        "      print(\"Train Data Visualise\")\n",
        "      fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 5))\n",
        "      ax1.plot(loss1_hist)\n",
        "      ax1.title.set_text('Train RPN Loss')\n",
        "      ax2.plot(loss2_hist)\n",
        "      ax2.title.set_text('Train Decider Loss')\n",
        "      ax3.plot(loss_hist)\n",
        "      ax3.title.set_text('Train Net Loss')\n",
        "      visualize_tensor(img, pred_box[pred_class.argmax(axis=1) != 0], gt_box)\n",
        "      for pred in pred_class.argmax(axis=1)[pred_class.argmax(axis=1) != 0]:\n",
        "        print(val_to_lab[int(pred)], end=', ')\n",
        "      print('')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    '''\n",
        "      Validation Code start\n",
        "    '''\n",
        "\n",
        "    if ((i+1)%evaluate_step == 0):\n",
        "      valid_loader = DataLoader(valid, batch_size=1, shuffle=True)\n",
        "      rpn.eval()\n",
        "      decider.eval()\n",
        "      print(\"-------------------------------\")\n",
        "      print(\"Evaluating valid sets\")\n",
        "      \n",
        "      test_model(iter(valid_loader).next()[0])\n",
        "      \n",
        "      valid_loss1 = 0\n",
        "      valid_loss2 = 0\n",
        "      valid_net_loss = 0\n",
        "      valid_run_count = 0\n",
        "      for valid_idx, valid_data in (enumerate(valid_loader,0)):\n",
        "        if (valid_idx > 25):\n",
        "          break\n",
        "        img, gt_box, labels = valid_data\n",
        "        loss1, loss2, pred_class, pred_box = single_image_pipeline(img.to(device), gt_box, labels, False)\n",
        "        net_loss = loss1 + loss2\n",
        "        \n",
        "        if not((math.isnan(net_loss)) or (math.isnan(loss1)) or (math.isnan(loss2))):\n",
        "          valid_loss1 += loss1.data\n",
        "          valid_loss2 += loss2.data\n",
        "          valid_net_loss += net_loss.data\n",
        "          valid_run_count += 1\n",
        "\n",
        "        if (((i+1)%(valid_visualise_step)) == 0  and (valid_idx == 0 )):\n",
        "          print(\"-------------------------------\")\n",
        "          print(\"-------------------------------\")\n",
        "          print(\"Validation Data Visualise -- \")\n",
        "          print(\"-------------------------------\")\n",
        "          print(\"-------------------------------\")\n",
        "          fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 5))\n",
        "          ax1.plot(valid_loss1_hist)\n",
        "          ax1.title.set_text('Valid RPN Loss')\n",
        "          ax2.plot(valid_loss2_hist)\n",
        "          ax2.title.set_text('Valid Decider Loss')\n",
        "          ax3.plot(valid_loss_hist)\n",
        "          ax3.title.set_text('Valid Net Loss')\n",
        "          visualize_tensor(img, pred_box[pred_class.argmax(axis=1) != 0], gt_box)\n",
        "          for pred in pred_class.argmax(axis=1)[pred_class.argmax(axis=1) != 0]:\n",
        "            print(val_to_lab[int(pred)], end=', ')\n",
        "          print('')\n",
        "          plt.show()\n",
        "        \n",
        "      valid_loss1_hist.append(valid_loss1/(valid_run_count + 1e-6))\n",
        "      valid_loss2_hist.append(valid_loss2/(valid_run_count+ 1e-6))\n",
        "      valid_loss_hist.append(valid_net_loss/(valid_run_count+ 1e-6))\n",
        "        \n",
        "      print(valid_run_count)\n",
        "      print(\"-------------------------------\")\n",
        "      print(\"-------------------------------\")\n",
        "      rpn.train()\n",
        "      decider.train()\n",
        "\n",
        "      '''\n",
        "      Validation Code end\n",
        "      '''\n",
        "  isFirstEpoch = False      \n",
        "  # print(\"-------------------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}